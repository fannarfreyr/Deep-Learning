# Experiment Log

## Experiment: multihead_v3_regularization
**Date:** 2025-11-13 14:49:54

**Changes Made:**
- Added L2 regularization (weight_decay=0.01)
- Switched batch size 64 → 128

**Reason for Change:**
Testing stronger regularization to improve generalization

**Training Metrics:**
- Epoch 1: Accuracy=0.7954, Loss=0.5501
- Epoch 2: Accuracy=0.7489, Loss=0.4815
- Epoch 3: Accuracy=0.8103, Loss=0.2136
- Epoch 4: Accuracy=0.7530, Loss=0.5166
- Epoch 5: Accuracy=0.7101, Loss=0.4452

**Final Results:**
Accuracy=0.7101, Loss=0.4452

**Notes / Insights:**
Even with errors, accuracy was promising. Need to test smaller LR.

---

## Experiment: multihead_v3_regularization
**Date:** 2025-11-13 14:53:50

**Changes Made:**
- Added L2 regularization (weight_decay=0.01)
- Switched batch size 64 → 128

**Reason for Change:**
Testing stronger regularization to improve generalization

**Training Metrics:**
- Epoch 1: Accuracy=0.8824, Loss=0.5900
- Epoch 2: Accuracy=0.8164, Loss=0.3107
- Epoch 3: Accuracy=0.8179, Loss=0.4040
- Epoch 4: Accuracy=0.9041, Loss=0.4281
- Epoch 5: Accuracy=0.8264, Loss=0.2190

**Final Results:**
Accuracy=0.8264, Loss=0.2190

**Notes / Insights:**
Hello world

---

## Experiment: multihead_v4_dropout_experiment
**Date:** 2025-11-13 14:59:37

**Changes Made:**
- Added Dropout 0.3
- Changed optimizer to AdamW

**Reason for Change:**
Try to reduce overfitting

**Training Metrics:**
- Epoch 1: Accuracy=0.8500, Loss=0.4000
- Epoch 2: Accuracy=0.9000, Loss=0.3000
- Epoch 3: Accuracy=0.9500, Loss=0.2000

**Final Results:**
Accuracy=0.95, Loss=0.25

**Notes / Insights:**
Promising results, model stable.

---

## Experiment: multihead_v4_dropout_experiment
**Date:** 2025-11-13 15:01:06

**Changes Made:**
- Added Dropout 0.3
- Changed optimizer to AdamW

**Reason for Change:**
Try to reduce overfitting

**Training Metrics:**
- Epoch 1: Accuracy=0.8500, Loss=0.4000
- Epoch 2: Accuracy=0.9000, Loss=0.3000
- Epoch 3: Accuracy=0.9500, Loss=0.2000

**Final Results:**
Accuracy=0.95, Loss=0.25

**Notes / Insights:**
Promising results, model stable.

---

## Experiment: multihead_v4_dropout_experiment
**Date:** 2025-11-13 15:03:27

**Changes Made:**
- Added Dropout 0.3
- Changed optimizer to AdamW

**Reason for Change:**
Try to reduce overfitting

**Training Metrics:**
- Epoch 1: Accuracy=0.8500, Loss=0.4000
- Epoch 2: Accuracy=0.9000, Loss=0.3000
- Epoch 3: Accuracy=0.9500, Loss=0.2000

**Final Results:**
Accuracy=0.95, Loss=0.25

**Notes / Insights:**
Promising results, model stable.

---

## Experiment: multihead_v6_dropout_lr
**Date:** 2025-11-13 15:47:26

**Changes Made:**
- Added dropout 0.3
- Adjusted learning rate schedule

**Reason for Change:**
Improve generalization and convergence speed

**Training Metrics:**
- Epoch 1: Accuracy=0.1657, Loss=4.2302

**Final Results:**
Best validation Accuracy=0.1657

**Notes / Insights:**
Training finished in 9m 38s

---

## Experiment: Name of experiment
**Date:** 2025-11-13 16:15:10

**Changes Made:**
Changes

**Reason for Change:**
Improve generalization and convergence speed

**Training Metrics:**
- Epoch 1: Accuracy=0.1799, Loss=4.2166
- Epoch 2: Accuracy=0.2523, Loss=3.7143

**Final Results:**
Best validation Accuracy=0.2523

**Notes / Insights:**
Training finished in 19m 18s

---

