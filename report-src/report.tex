\documentclass{article}



\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}



\title{A template for the \emph{arxiv} style}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

\author{ \href{https://orcid.org/0000-0000-0000-0000}{\hspace{1mm}David S.~Hippocampus}\thanks{Use footnote for providing further
		information about author (webpage, alternative
		address)---\emph{not} for acknowledging funding agencies.} \\
	Department of Computer Science\\
	Cranberry-Lemon University\\
	Pittsburgh, PA 15213 \\
	\texttt{hippo@cs.cranberry-lemon.edu} \\
	%% examples of more authors
	\And
	\href{https://orcid.org/0000-0000-0000-0000}{\hspace{1mm}Elias D.~Striatum} \\
	Department of Electrical Engineering\\
	Mount-Sheikh University\\
	Santa Narimana, Levand \\
	\texttt{stariate@ee.mount-sheikh.edu} \\
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}

% Uncomment to remove the date
%\date{}

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{Technical Report}
%\renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
	\lipsum[1]
\end{abstract}


% keywords can be removed
\keywords{First keyword \and Second keyword \and More}


\section{Introduction}

Fine-grained vehicle classification, which differentiates between various types, makes, and models of cars, presents significant challenges. This difficulty arises from the large number of visually similar vehicle categories, variations in lighting and weather conditions, occlusions, and the diverse viewpoints from which vehicles may be captured. As a result, building a robust vehicle classification system requires a model capable of recognizing subtle visual cues while generalizing across highly variable real-world conditions.

In this project, we aim to explore and compare different strategies for hierarchical image classification on the Stanford Cars Dataset (Krause et al., 2013). The dataset contains 16,185 high-resolution images labeled with three hierarchical attributes: vehicle make, vehicle type, and vehicle model. These labels naturally form a multi-level taxonomy, making the dataset well-suited for studying hierarchical classification approaches.

Our baseline approach uses a ResNet-50 model pretrained on ImageNet as the backbone architecture. Initially, we treat the problem as a flat classification task by predicting the complete car label (containing make, type, and model) as a single class among 196 possible categories. This serves as our starting point for evaluating how well a standard single-head classifier performs on fine-grained classification without explicit hierarchical structure.

Building on this baseline, the primary objective of this project is to investigate how different hierarchical output designs influence classification performance. Specifically, we focus on three classification strategies:

\begin{enumerate}
    \item \textbf{Single-head (flat) classifier:}
    Predicts the entire label (make, type, and model) as one combined class. This ignores the hierarchical relationships.
    \item \textbf{Two-head classifier:}
    One head predicts the vehicle make, while the second head predicts the combined type+model label. 
    \item \textbf{Three-head classifier:}
    Predicts make, type, and model independently using three parallel classification heads.
\end{enumerate}   

After we evaluated these 3 variants, we will further enhance their performance using targeted regularization and optimization strategies. These include data augmentation, dropout, weight decay, and learning-rate adjustments, all of which can help reduce overfitting.

\section{Related Work}

\section{Methods}
\subsection{Dataset and Pre-Processing}
\begin{itemize}
	\item Describe the dataset size 
	\item Define your hierarchy: Make (Easy, e.g., BMW) $\to$ Type (Medium, e.g., Sedan) $\to$ Model (Hard, e.g., 3-Series).
\end{itemize}

\subsection{Model Architecture}
\begin{itemize}
	\item \textbf{Backbone:} ResNet-50 (pretrained on ImageNet)
	\item \textbf{Multi-Task Heads:} Describe how you split the network after the backbone into 2 heads (Make/Model) or 3 heads (Make/Type/Model).
	\item \textbf{Multi-Head Architecture:} Describe splitting the network after the backbone into specific heads (Make, Type, Model). Explain the loss function (e.g., $L_{total} = L_{make} + L_{type} + L_{model}$)
\end{itemize}

\subsection{Training Strategies}
\begin{itemize}
	\item \textbf{Baseline:} Frozen vs. Unfrozen backbones.
	\item \textbf{Data Augmentation:} Describe the geometric/color transforms used to prevent overfitting (Experiment Multihead\_two\_heads\_with\_data\_augmentation).
	\item \textbf{Curriculum Learning:} Explain the strategy of freezing the "Hard" head (Model) for the first 5 epochs to let the backbone learn generic shapes (Type) and brands (Make) first.
	\item \textbf{Hierarchical Label Smoothing (HLS):} Explain replacing hard targets (0/1) with soft targets to penalize "near misses" less than "far misses".
\end{itemize}

\section{Results}
Organize results by "Research Question" rather than date.

\subsection{Establishing the "Hierarchy Gap" (Baselines)}
\begin{itemize}
	\item \textbf{Objective:} Show that flat classifiers fail to capture relationships
	\item \textbf{Data:} Compare the Frozen Baseline (~42\% Acc) vs. Unfrozen (~76\% Acc).
	\item \textbf{Key Finding:} Even with decent accuracy, the "Gap" between Make Accuracy (85.5\%) and Model Accuracy (75.9\%) was nearly 10\%, indicating the model was guessing Models without knowing the Brand.
	
\end{itemize}

\subsection{The Impact of Regularization \& Class Balancing}
\begin{itemize}
	\item \textbf{Objective:} Solving the overfitting problem. 
	\item \textbf{Data:} Show the jump from ~77\% (Phase 3) to 86.6\% (Phase 4) just by adding Data Augmentation.
	\item \textbf{Observation:} Note that Class Balancing (Phase 5) helped rare classes but slightly hurt overall consistency (The "Robin Hood" effect).
	
\end{itemize}

\subsection{Architectural Ablation: The Necessity of "Type"}
\begin{itemize}
	\item \textbf{Objective:} Solving the overfitting problem. 
	\item \textbf{Data:} Compare Experiment 8 (2-Head Curriculum: 86.49\%) against Experiment 7 (3-Head Curriculum: 86.15\% - initial) and Experiment 9/10 (Final 3-Head: 89\%).
	\item \textbf{Key Finding:} While 3-Head initially struggled due to interference, once optimized (see next section), it outperformed the 2-Head approach, proving that the "Type" layer acts as a necessary semantic bridge.
		
\end{itemize}

\subsection{Optimization Dynamics: Interference vs. Curriculum}
\begin{itemize}
	\item \textbf{Objective:} Solving the "Task Interference" problem in Multi-Task Learning.
	\item \textbf{Data:} Contrast Experiment 6 (3-Head No Curriculum: 85.36\%) vs. Experiment 7 (3-Head With Curriculum: 86.15\%).
	\item \textbf{Key Finding:} Without curriculum, gradients conflicted. Freezing the hard head allowed the backbone to learn stable features first.
		
\end{itemize}

\subsection{SOTA Performance: Label Smoothing \& LR Scheduler}
\begin{itemize}
	\item \textbf{Objective:} Pushing the limit. 
	\item \textbf{Data:} Present the final model (Experiment 10) achieving 89.07\% Top-1 Accuracy.
	\item \textbf{Key Finding:} The Scheduler + HLS closed the consistency gap significantly.
		
\end{itemize}


\section{Discussion}
This is the most critical section for grading. Interpret the patterns.

\subsection{The "Frankenstein Car" Problem}
\begin{itemize}
	\item Discuss Hierarchical Consistency.
	\item In early experiments, consistency was low (~90\%). The model would predict "Toyota" (Make) and "Honda Civic" (Model).
	\item By Experiment 9 (HLS), consistency reached 96.6\%, and finally 97.66\% with the scheduler. This proves the model learned the taxonomy, not just pixel patterns.
\end{itemize}


\subsection{The Role of "Type" as Scaffolding}
\begin{itemize}
	\item Analyze why 3\_head\_curriculum eventually beat 2\_head\_curriculum.
	\item The "Type" head (Sedan, SUV, Coupe) provides Intermediate Scaffolding. It is easier to learn than "Model" but provides more structural information than "Make." It bridges the semantic gap.
\end{itemize}


\subsection{The "Free Lunch" of Training Schedules}
\begin{itemize}
	\item Discuss the final experiment (LR Scheduler).
	\item ou gained ~1.5\% accuracy (87.9\% $\to$ 89.07\%) just by changing the learning rate schedule. This indicates the architecture was sound, but the optimizer needed "fine-grained" control to settle into the sharp minima of the loss landscape.
\end{itemize}

\section{Conclusion}
Conclude that while ResNet50 is powerful, structuring the learning process (Curriculum) and enforcing taxonomy (Multi-head + HLS) creates a model that is not only more accurate but logically robust.





\section{Headings: first level}
\label{sec:headings}

\lipsum[4] See Section \ref{sec:headings}.

\subsection{Headings: second level}
\lipsum[5]
\begin{equation}
	\xi _{ij}(t)=P(x_{t}=i,x_{t+1}=j|y,v,w;\theta)= {\frac {\alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}{\sum _{i=1}^{N} \sum _{j=1}^{N} \alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}}
\end{equation}

\subsubsection{Headings: third level}
\lipsum[6]

\paragraph{Paragraph}
\lipsum[7]



\section{Examples of citations, figures, tables, references}
\label{sec:others}

\subsection{Citations}
Citations use \verb+natbib+. The documentation may be found at
\begin{center}
	\url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}

Here is an example usage of the two main commands (\verb+citet+ and \verb+citep+): Some people thought a thing \citep{kour2014real, hadash2018estimate} but other people thought something else \citep{kour2014fast}. Many people have speculated that if we knew exactly why \citet{kour2014fast} thought this\dots

\subsection{Figures}
\lipsum[10]
See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}
\lipsum[11]

\begin{figure}
	\centering
	\fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
	\caption{Sample figure caption.}
	\label{fig:fig1}
\end{figure}

\subsection{Tables}
See awesome Table~\ref{tab:table}.

The documentation for \verb+booktabs+ (`Publication quality tables in LaTeX') is available from:
\begin{center}
	\url{https://www.ctan.org/pkg/booktabs}
\end{center}


\begin{table}
	\caption{Sample table title}
	\centering
	\begin{tabular}{lll}
		\toprule
		\multicolumn{2}{c}{Part}                   \\
		\cmidrule(r){1-2}
		Name     & Description     & Size ($\mu$m) \\
		\midrule
		Dendrite & Input terminal  & $\sim$100     \\
		Axon     & Output terminal & $\sim$10      \\
		Soma     & Cell body       & up to $10^6$  \\
		\bottomrule
	\end{tabular}
	\label{tab:table}
\end{table}

\subsection{Lists}
\begin{itemize}
	\item Lorem ipsum dolor sit amet
	\item consectetur adipiscing elit.
	\item Aliquam dignissim blandit est, in dictum tortor gravida eget. In ac rutrum magna.
\end{itemize}


\bibliographystyle{unsrtnat}
\bibliography{references}  %%% Uncomment this line and comment out the ``thebibliography'' section below to use the external .bib file (using bibtex) .


%%% Uncomment this section and comment out the \bibliography{references} line above to use inline references.
% \begin{thebibliography}{1}

% 	\bibitem{kour2014real}
% 	George Kour and Raid Saabne.
% 	\newblock Real-time segmentation of on-line handwritten arabic script.
% 	\newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
% 			International Conference on}, pages 417--422. IEEE, 2014.

% 	\bibitem{kour2014fast}
% 	George Kour and Raid Saabne.
% 	\newblock Fast classification of handwritten on-line arabic characters.
% 	\newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
% 			International Conference of}, pages 312--318. IEEE, 2014.

% 	\bibitem{hadash2018estimate}
% 	Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
% 	Jacovi.
% 	\newblock Estimate and replace: A novel approach to integrating deep neural
% 	networks with existing applications.
% 	\newblock {\em arXiv preprint arXiv:1804.09028}, 2018.

% \end{thebibliography}


\end{document}
